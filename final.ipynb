{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#### Importing the Relevant Librairies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['PYTHONHASHSEED']=str(1)\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor,  StackingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import metrics\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data): \n",
    "    #wheter there is a personal url or not\n",
    "    data[\"Personal URL\"] = 1 - data[\"Personal URL\"].isna()\n",
    "    \n",
    "    #getting a month since creation column\n",
    "    data[\"Profile Creation Timestamp\"] = pd.to_datetime(data[\"Profile Creation Timestamp\"])\n",
    "    data[\"Months Since Creation\"] = round(\n",
    "        (max(pd.to_datetime(data[\"Profile Creation Timestamp\"])).tz_localize(None)\n",
    "         - pd.to_datetime(data[\"Profile Creation Timestamp\"]).dt.tz_localize(None)).dt.days/30)\n",
    "    \n",
    "    #changing the verification status to a number\n",
    "    data[\"Profile Verification Status\"]  = np.where(data[\"Profile Verification Status\"]==\"Verified\", 1, 0)\n",
    "    \n",
    "    data[\"Profile Cover Image Status\"].fillna('NA', inplace=True)\n",
    "    \n",
    "    #We may want to select individual colours in our subset.\n",
    "    data[\"Profile Theme Color\"] = data[\"Profile Theme Color\"].apply(lambda x: \"thc-\"+x if x in frequent_theme_colors else \"others\")\n",
    "    data = data.merge(pd.get_dummies(data[\"Profile Theme Color\"]), left_index = True, right_index = True)\n",
    "    data[\"Profile Page Color\"] = data[\"Profile Page Color\"].apply(lambda x: \"pc-\"+x if x in frequent_page_colors else \"others\")\n",
    "    data = data.merge(pd.get_dummies(data[\"Profile Page Color\"]), left_index = True, right_index = True)\n",
    "    data[\"Profile Text Color\"] = data[\"Profile Text Color\"].apply(lambda x: \"txc-\"+x if x in frequent_text_colors else \"others\")\n",
    "    data = data.merge(pd.get_dummies(data[\"Profile Text Color\"]), left_index = True, right_index = True)\n",
    "    \n",
    "    #bool to int because we want it to remain a OHE\n",
    "    data[\"Is Profile View Size Customized?\"] = data[\"Is Profile View Size Customized?\"].astype(int)\n",
    "    \n",
    "    #remove capitalisation\n",
    "    data[\"Location Public Visibility\"]  = data[\"Location Public Visibility\"].str.lower()\n",
    "    \n",
    "    #Locales are frequent elements from a bag of words extracted from Location.\n",
    "    data[\"Location\"] = data[\"Location\"].fillna(\"unknown\")\n",
    "    locale_list = ['unknown',  'Canada', 'Chile', 'Jakarta', 'Arabia',  'Colombia', 'Venezuela',\n",
    "       'Argentina', 'Australia','España',\n",
    "       'England', 'France', 'Paris', 'London',\n",
    "    'Worldwide',\n",
    "       'Indonesia', 'México', 'United', 'California', 'Washington','Brasil', 'India', 'New York', 'Los Angeles']\n",
    "    for locale in locale_list:\n",
    "        data[locale] = data[\"Location\"].apply(lambda x: locale.lower() in x.lower() if not pd.isna(x) else False).astype(int)\n",
    "    \n",
    "    #prepare location for target mean encoding\n",
    "    def locale_cat(location):\n",
    "        replacement = 'other'\n",
    "        for x in locale_list:\n",
    "            if x.lower() in location.lower():\n",
    "                replacement = x.lower()\n",
    "        return replacement\n",
    "        \n",
    "    data[\"Locale_TME\"] = data[\"Location\"].apply(locale_cat)\n",
    "    \n",
    "    #prepare language for target mean encoding\n",
    "    for language in common_languages:\n",
    "        data[language] = (data[\"User Language\"]==language).astype(int)\n",
    "        \n",
    "    data[\"Language_TME\"] = data[\"User Language\"].apply(lambda x: x if x in common_languages else \"others\")\n",
    "        \n",
    "    #prepare timezones for target mean encoding\n",
    "    for zone in time_zones:\n",
    "        data[\"tz - \" + zone] = (data[\"User Time Zone\"] == zone).astype(int)\n",
    "    \n",
    "    data[\"TZ_TME\"] = data[\"User Time Zone\"].apply(lambda x: x if x in time_zones else \"others\")\n",
    "    \n",
    "    #Assuming we will be treating UTC Offset as categorical\n",
    "    data[\"UTC Offset\"] = data[\"UTC Offset\"].fillna(0.5)\n",
    "    data[\"UTC Offset\"] = data[\"UTC Offset\"].astype(str)+\"aaa\"\n",
    "\n",
    "    #we noticed \" \" was different from \"unknown\" in terms of average/median\n",
    "    data.loc[data[\"Profile Category\"] == \" \", \"Profile Category\"] = \"empty\"\n",
    "    \n",
    "    # for avg visit duration an cliks, infer nan with median per profile categories\n",
    "    data[\"Avg Daily Profile Visit Duration in seconds\"] = data.groupby([\"Profile Category\"])[\"Avg Daily Profile Visit Duration in seconds\"].transform(\n",
    "        lambda x: x.fillna(x.median()))\n",
    "    data[\"Avg Daily Profile Clicks\"] = data.groupby([\"Profile Category\"])[\"Avg Daily Profile Clicks\"].transform(\n",
    "        lambda x: x.fillna(x.median()))\n",
    "    \n",
    "    #estimated clicks = avg daily profile clicks times day, but we just did \"times months\", it's effectively the same after scaling\n",
    "    data[\"Estimated Clicks\"] = data[\"Avg Daily Profile Clicks\"]*data[\"Months Since Creation\"]\n",
    "    \n",
    "    # Log scales\n",
    "    data[\"Num of Direct Messages\"] = np.log(data[\"Num of Direct Messages\"]+1)\n",
    "    data[\"Num of Status Updates\"] = np.log(data[\"Num of Status Updates\"]+1)\n",
    "    data[\"Num of Followers\"] = np.log(data[\"Num of Followers\"]+1)\n",
    "    data[\"Num of People Following\"] = np.log(data[\"Num of People Following\"]+1)\n",
    "    data[\"Avg Daily Profile Clicks\"] = np.log(data[\"Avg Daily Profile Clicks\"]+1)\n",
    "    data[\"Estimated Clicks\"] = np.log(data[\"Estimated Clicks\"]+1)\n",
    "\n",
    "    return data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the preprocess_data() function on the relevant collumn of the dataframe and select the relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Num of Profile Likes\"\n",
    "\n",
    "df = pd.read_csv(\"train.csv\", index_col = 0)\n",
    "test = pd.read_csv(\"test.csv\", index_col = 0)\n",
    "\n",
    "#keeping the index for the final submission, in case order of indices matter\n",
    "final_index = test.index\n",
    "\n",
    "#sort indices because it's less confusing that way\n",
    "df.sort_index(inplace=True)\n",
    "test.sort_index(inplace=True)\n",
    "\n",
    "#get most frequent colors/languages/etc\n",
    "frequent_theme_colors = set(df[\"Profile Theme Color\"].value_counts().head(10).index)\n",
    "frequent_page_colors = set(df[\"Profile Page Color\"].value_counts().head(10).index)\n",
    "frequent_text_colors = set(df[\"Profile Text Color\"].value_counts().head(10).index)\n",
    "common_languages = df[\"User Language\"].value_counts().head(20).index\n",
    "time_zones = df[\"User Time Zone\"].value_counts().head(25).index\n",
    "\n",
    "#preprocessing\n",
    "X = df.loc[:, df.columns.difference([target])]\n",
    "y = df.loc[:, target]\n",
    "X = preprocess_data(X.copy())\n",
    "X_test = preprocess_data(test.copy())\n",
    "\n",
    "\n",
    "#subset of handpicked features we care about\n",
    "subset1 = ['Personal URL', 'Profile Cover Image Status',\n",
    "          'Profile Verification Status', 'Is Profile View Size Customized?',\n",
    "          'UTC Offset', 'Location Public Visibility', 'Num of Followers',\n",
    "          'Num of People Following', 'Num of Status Updates',\n",
    "          'Num of Direct Messages', 'Avg Daily Profile Clicks',\n",
    "          'Months Since Creation',\n",
    "          'Estimated Clicks', 'Profile Category',\n",
    "           'Profile Theme Color', 'Locale_TME',\n",
    "          \"Language_TME\", \"TZ_TME\"\n",
    "         ]\n",
    "\n",
    "\n",
    "X1 = X.loc[:, subset1]\n",
    "X1_test = X_test.loc[:, subset1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to preprocess the train and the validation set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lim indicates outlier removal through winsorization. Ended up not used in final submission\n",
    "def process_train_test_winz(X_train, y_train, X_test, lim = 0.0):\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    medians = X_train.median(axis=0).copy()\n",
    "    X_train = X_train.fillna(medians)\n",
    "    X_test = X_test.fillna(medians)\n",
    "    \n",
    "    cat_cols = X_train.select_dtypes(exclude=np.number).columns\n",
    "    \n",
    "    for cat in cat_cols:\n",
    "        encoder = ce.TargetEncoder()\n",
    "        X_train[cat] = encoder.fit_transform(X_train[cat], np.log(y_train.values+1))\n",
    "        X_test[cat] = encoder.transform(X_test[cat])#In case of missing catedory (from X_train), the target mean.\n",
    "        \n",
    "    for col in ['Num of Followers',\n",
    "          'Num of People Following', 'Num of Status Updates',\n",
    "          'Num of Direct Messages', 'Avg Daily Profile Clicks']: \n",
    "        X_train[col] = stats.mstats.winsorize(X_train[col], limits=lim)\n",
    "        X_test[col] = stats.mstats.winsorize(X_test[col], limits=lim)\n",
    "    \n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSLE : 1.4131976664135983\n"
     ]
    }
   ],
   "source": [
    "#Necessary imports and random seeding for reproducibility.\n",
    "\n",
    "os.environ['PYTHONHASHSEED']=str(1)\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "x, x_test = process_train_test_winz(X1, y, X1_test)\n",
    "estimators = [('lgb', LGBMRegressor(\n",
    "                          learning_rate=0.02,\n",
    "                          max_bin=135,\n",
    "                          min_data_in_leaf=25,\n",
    "                          num_iterations=500,\n",
    "                          num_leaves=11,\n",
    "                          reg_alpha=0.2,\n",
    "                          min_child_samples=None)),\n",
    "              ('svr', SVR(C=1.9, epsilon=0.8, kernel='rbf')),\n",
    "              ('rf', RandomForestRegressor(\n",
    "                         **{'bootstrap': True,\n",
    "                            'ccp_alpha': 0.0,\n",
    "                            'max_depth': 12,\n",
    "                            'max_features': 'auto',\n",
    "                            'max_leaf_nodes': None,\n",
    "                            'min_impurity_decrease': 0.0,\n",
    "                            'min_samples_leaf': 2,\n",
    "                            'min_samples_split': 3,\n",
    "                            'min_weight_fraction_leaf': 0.0,\n",
    "                            'n_estimators': 100})),\n",
    "              ('xgb', XGBRegressor(\n",
    "                          objective='reg:squarederror',\n",
    "                          **{'learning_rate' : 0.05,\n",
    "                             'gamma': 0.1,\n",
    "                             'max_depth': 5,\n",
    "                             'min_child_weight': 1,\n",
    "                             'subsample' : 1,\n",
    "                             'n_estimators': 180,\n",
    "                             'colsample_bytree' : 0.33,\n",
    "                             'alpha': 2.5}))]\n",
    "\n",
    "m = StackingRegressor(estimators=estimators, cv=50)\n",
    "m.fit(x, np.log(y+1))\n",
    "pred_train = (np.exp(m.predict(x))-1).clip(min=0)\n",
    "pred_test = (np.exp(m.predict(x_test))-1).clip(min=0)\n",
    "print(\"Training RMSLE :\", np.sqrt(metrics.mean_squared_log_error(y.values, pred_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Last Verification to make sure the predictions are in the right format and close to what we should expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check diff of length in test inputs and preds: 0\n",
      "Median number of likes in training:   1370.0\n",
      "Median number of likes in prediction: 1362.7082050432873\n",
      "Check min predicted value: Predicted    0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Check diff of length in test inputs and preds:\", len(X_test) - len(pred_test))\n",
    "print(\"Median number of likes in training:  \", np.median(y))\n",
    "print(\"Median number of likes in prediction:\", np.median(pred_test))\n",
    "result = test.copy()\n",
    "result[\"Predicted\"] = pred_test\n",
    "result = result.loc[final_index]\n",
    "print(\"Check min predicted value:\", result[[\"Predicted\"]].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making the file into the dataframe for submission on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[[\"Predicted\"]].to_csv(\"pred.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
